import os
import json
import requests
from bs4 import BeautifulSoup
import re
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import sys
import io
import logging
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_FILE = os.path.join(BASE_DIR, "parser.log")

logger = logging.getLogger("parser_logger")
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler(LOG_FILE, encoding="utf-8")
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
file_handler.setFormatter(formatter)

if not logger.hasHandlers():
    logger.addHandler(file_handler)

def download_image(src, path):
    try:
        r = requests.get(src, headers={"User-Agent": "Mozilla/5.0"})
        if r.status_code == 200:
            with open(path, "wb") as f:
                f.write(r.content)
            return True
    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {src}: {e}")
    return False

def extract_hd_images_from_json(html):
    soup = BeautifulSoup(html, "html.parser")
    scripts = soup.find_all("script", type="application/json")

    urls = set()

    for script in scripts:
        try:
            data = json.loads(script.string)
            data_str = json.dumps(data)
            for match in data_str.split('"'):
                if "bstatic.com" in match and ("1024" in match or "max" in match) and ".jpg" in match:
                    urls.add(match)
        except Exception:
            continue

    return list(urls)

def normalize(text):
    return re.sub(r"[\s\*\-\(\)_]", "", text.lower())

def safe_download(url, path, retries=3, timeout=15):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –∏ —Ç–∞–π–º–∞—É—Ç–æ–º"""
    for attempt in range(retries):
        try:
            r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, stream=True, timeout=timeout)
            if r.status_code == 200:
                with open(path, "wb") as f:
                    for chunk in r.iter_content(1024):
                        f.write(chunk)
                return True
            else:
                logger.warning(f"‚ö†Ô∏è {url} –≤–µ—Ä–Ω—É–ª {r.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
        time.sleep(1)  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
    return False

def scrape_booking_vlite_plus(url, folder_name="downloaded_images_plus", limit=180):
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)

    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=20)
    except Exception as e:
        logger.info(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}")
        return
    if r.status_code != 200:
        logger.info(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {r.status_code}")
        return

    urls = extract_hd_images_from_json(r.text)

    if not urls:
        logger.info("‚ö†Ô∏è HD-—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ–∫
    urls = urls[:limit]

    count = 0
    for i, src in enumerate(urls):
        filename = os.path.join(folder_name, f"photo_{i+1}.jpg")
        if safe_download(src, filename):
            logger.info(f"‚úÖ –°–∫–∞—á–∞–Ω–æ: {filename}")
            count += 1

    logger.info(f"üì¶ –í—Å–µ–≥–æ —Å–∫–∞—á–∞–Ω–æ: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ª–∏–º–∏—Ç {limit})")

def extract_description(url, folder_path):
    import requests
    from bs4 import BeautifulSoup
    import os

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7"
    }

    def fetch_description(test_url, lang="ru"):
        try:
            resp = requests.get(test_url, headers=headers, timeout=20)
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, "html.parser")
                selectors = [
                    "p[data-testid='property-description']",
                    "div#property_description_content",
                    "div[data-capla-component='property-description']",
                    "section[data-testid='property-description']",
                ]
                for sel in selectors:
                    block = soup.select_one(sel)
                    if block:
                        text = block.get_text(" ", strip=True)
                        if text and len(text) > 30:
                            logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ ({lang}) —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {sel}")
                            return text
            else:
                logger.warning(f"‚ö†Ô∏è Booking –≤–µ—Ä–Ω—É–ª {resp.status_code} –¥–ª—è {test_url}")
        except Exception as e:
            logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –æ–ø–∏—Å–∞–Ω–∏—è ({lang}): {e}")
        return None

    # 1. –ø—Ä–æ–±—É–µ–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    if "?lang=" not in url:
        url_ru = url + ("&lang=ru" if "?" in url else "?lang=ru")
    else:
        url_ru = url
    description = fetch_description(url_ru, "ru")

    # 2. –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –ø—Ä–æ–±—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
    if not description:
        url_en = url.split("?")[0] + "?lang=en"
        description = fetch_description(url_en, "en")

    if not description:
        description = "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"

    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    os.makedirs(folder_path, exist_ok=True)
    desc_file = os.path.join(folder_path, "description.txt")
    try:
        with open(desc_file, "w", encoding="utf-8") as f:
            f.write(description)
        logger.info(f"üíæ –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {desc_file}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ description.txt: {e}")

    return description

