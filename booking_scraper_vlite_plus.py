import os
import json
import requests
from bs4 import BeautifulSoup
import re
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import sys
import io
import logging
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_FILE = os.path.join(BASE_DIR, "parser.log")

logger = logging.getLogger("parser_logger")
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler(LOG_FILE, encoding="utf-8")
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
file_handler.setFormatter(formatter)

if not logger.hasHandlers():
    logger.addHandler(file_handler)

def download_image(src, path):
    try:
        r = requests.get(src, headers={"User-Agent": "Mozilla/5.0"})
        if r.status_code == 200:
            with open(path, "wb") as f:
                f.write(r.content)
            return True
    except Exception as e:
        logger.info(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {src}: {e}")
    return False

def extract_hd_images_from_json(html):
    soup = BeautifulSoup(html, "html.parser")
    scripts = soup.find_all("script", type="application/json")

    urls = set()

    for script in scripts:
        try:
            data = json.loads(script.string)
            data_str = json.dumps(data)
            for match in data_str.split('"'):
                if "bstatic.com" in match and ("1024" in match or "max" in match) and ".jpg" in match:
                    urls.add(match)
        except Exception:
            continue

    return list(urls)

def normalize(text):
    return re.sub(r"[\s\*\-\(\)_]", "", text.lower())

def scrape_booking_vlite_plus(url, folder_name="downloaded_images_plus"):
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)

    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers)
    if r.status_code != 200:
        logger.info("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
        return

    urls = extract_hd_images_from_json(r.text)

    if not urls:
        logger.info("‚ö†Ô∏è HD-—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    count = 0
    for i, src in enumerate(urls):
        filename = os.path.join(folder_name, f"photo_{i+1}.jpg")
        if download_image(src, filename):
            logger.info(f"‚úÖ –°–∫–∞—á–∞–Ω–æ: {filename}")
            count += 1

    logger.info(f"üì¶ –í—Å–µ–≥–æ —Å–∫–∞—á–∞–Ω–æ: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

def extract_description(url, folder_path):
    try:
        chrome_options = Options()
        chrome_options.page_load_strategy = "eager"  # —É—Å–∫–æ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1280,1024")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
        chrome_options.add_argument("--disable-notifications")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        chromedriver_path = os.path.join(os.path.dirname(__file__), "chromedriver.exe")
        service = Service(executable_path=chromedriver_path)
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(60)

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
        if "?lang=" not in url:
            if "?" in url:
                url += "&lang=ru"
            else:
                url += "?lang=ru"

        logger.info(f"üåê –ò—Ç–æ–≥–æ–≤—ã–π URL: {url}")
        driver.get(url)

        os.makedirs(folder_path, exist_ok=True)
        description = "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"

        try:
            # –ñ–¥—ë–º –∏–º–µ–Ω–Ω–æ <p data-testid="property-description">
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "p[data-testid='property-description']"))
            )
            element = driver.find_element(By.CSS_SELECTOR, "p[data-testid='property-description']")
            driver.execute_script("arguments[0].scrollIntoView(true);", element)
            tmp_text = element.text.strip()

            if tmp_text:
                description = tmp_text
                logger.info("‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
            else:
                logger.warning("‚ö† –û–ø–∏—Å–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ'.")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª description.txt
            with open(os.path.join(folder_path, "description.txt"), "w", encoding="utf-8") as f:
                f.write(description)
            logger.info(f"üíæ –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {os.path.join(folder_path, 'description.txt')}")

            # –û–±–Ω–æ–≤–ª—è–µ–º filter.json
            script_dir = os.path.dirname(os.path.abspath(__file__))
            filter_path = os.path.join(script_dir, "data", "filter.json")

            data = []
            if os.path.exists(filter_path):
                try:
                    with open(filter_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                except Exception as e:
                    logger.info(f"‚ö† –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è filter.json: {e}")

            hotel_name = os.path.basename(folder_path)
            hotel_key = normalize(hotel_name)
            updated = False

            for entry in data:
                entry_name = normalize(entry.get("hotel", ""))
                if hotel_key in entry_name or entry_name in hotel_key:
                    entry["description"] = description
                    logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –æ—Ç–µ–ª—è: {entry['hotel']}")
                    updated = True
                    break

            if not updated:
                data.append({"hotel": hotel_name, "description": description})
                logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –æ—Ç–µ–ª—å: {hotel_name}")

            try:
                with open(filter_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                logger.info("üìÅ filter.json —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª—ë–Ω.")
            except Exception as e:
                logger.info(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ filter.json: {e}")

        except Exception as e:
            logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –±–ª–æ–∫ –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            # –î–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
            with open(os.path.join(folder_path, "description.txt"), "w", encoding="utf-8") as f:
                f.write(description)

        driver.quit()

    except Exception as e:
        logger.info(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–Ω–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è: {e}")

    if __name__ == "__main__": 
        return